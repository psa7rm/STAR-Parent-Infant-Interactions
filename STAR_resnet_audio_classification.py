# -*- coding: utf-8 -*-
"""Copy of Main_Capstone_Will.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12-rmj2wxZO_UbgeVJEdm0fp9RjbRlAld

Notebook Written in Google Colab
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
import tensorflow_hub as hub
from tensorflow.keras import layers, models
import glob

import torch
print(torch.__version__)
import torch.nn.functional as F
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader, random_split
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from functools import partial

import tensorflow as tf
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.preprocessing import image_dataset_from_directory

"""# Import and Transform Data"""

#Change data directories to what is relevant on your drive

train_dataset = image_dataset_from_directory(
    "/content/drive/My Drive/CapstoneNewExcelFiles/ImageFiles/TrainSmall/",
    batch_size=1,  # Adjust batch size according to your need
    image_size=(224, 224),  # ResNet50 default image size is (224, 224), but you can resize if needed
    shuffle=True,
    label_mode='int',  # 'int' for integer labels; 'categorical' for one-hot encoded labels
    seed=123  # Seed for shuffling and transformations
)

# For the validation dataset, make sure to set the same seed and disable shuffle if it's for evaluation
validation_dataset = image_dataset_from_directory(
    "/content/drive/My Drive/CapstoneNewExcelFiles/ImageFiles/TestSmall/",  # Ensure you have this directory for validation
    batch_size=1,
    image_size=(224, 224),
    shuffle= True,
    label_mode='int',
    seed=123
)



"""# Models"""

from tensorflow.keras.applications.resnet import preprocess_input

def preprocess_data(image, label):
    return preprocess_input(image), label

train_dataset = train_dataset.map(preprocess_data)
validation_dataset = validation_dataset.map(preprocess_data)

# from tensorflow.keras.applications import ResNet50
# from tensorflow.keras.layers import GlobalAveragePooling2D, Dense
# from tensorflow.keras.models import Model

# # Load ResNet50 base model
# base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
# base_model.trainable = False  # Freeze the base model

# # Create new model on top
# x = GlobalAveragePooling2D()(base_model.output)
# x = Dense(1024, activation='relu')(x)
# output = Dense(3, activation='softmax')(x)  # Assuming you have 3 classes
# model = Model(inputs=base_model.input, outputs=output)

# # Compile the model
# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam  # Import the Adam optimizer

# Load ResNet50 base model
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
base_model.trainable = False  # Freeze the base model

# Create new model on top
x = GlobalAveragePooling2D()(base_model.output)
x = Dense(1024, activation='relu')(x)
output = Dense(3, activation='softmax')(x)  # Assuming you have 3 classes
model = Model(inputs=base_model.input, outputs=output)

# Specify the learning rate
learning_rate = 0.001  # You can change this value to whatever you prefer

# Compile the model using an optimizer object with a specified learning rate
model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# # Train the model
# model.fit(train_dataset, epochs=10, validation_data=validation_dataset)

history = model.fit(
    train_dataset,
    epochs=50,
    validation_data=validation_dataset
)

import matplotlib.pyplot as plt

# Plotting the training and validation loss
plt.figure(figsize=(8, 5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss Per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

import matplotlib.pyplot as plt

# Ensure that 'accuracy' is included in your metrics during model compilation
# For example:
# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Plotting the training and validation accuracy
plt.figure(figsize=(8, 5))
plt.plot(history.history['accuracy'], label='Training Accuracy')  # Use 'acc' instead of 'accuracy' if using TensorFlow 1.x
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')  # Use 'val_acc' if using TensorFlow 1.x
plt.title('Training and Validation Accuracy Per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

# Sample prediction (replace with your actual test data loading if it's different)
y_pred = model.predict(validation_dataset)
y_pred_classes = y_pred.argmax(axis=-1)  # Get the predicted class labels

validation_dataset_values = []
for x, y in validation_dataset:
  validation_dataset_values.append(y.numpy())

validation_dataset_values = np.concatenate(validation_dataset_values)

from sklearn.metrics import confusion_matrix
import pandas as pd

# Create the confusion matrix
cm = confusion_matrix(validation_dataset_values, y_pred_classes)

# Convert the confusion matrix to a DataFrame
df_cm = pd.DataFrame(cm, columns=['Infant', 'Parent', 'Other'], index=['Actual Infant', 'Actual Parent', 'Actual Other'])

# Display the confusion matrix
print(df_cm)

import seaborn as sns  # This imports the seaborn library and aliases it as 'sns'


conf_matrix = confusion_matrix(y_pred_classes, validation_dataset_values)
# Plot confusion matrix
plt.figure(figsize=(3, 3))
sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix for Audio Classification Model')
plt.show()

"""Model 2 - decrease learning rates"""

# Load ResNet50 base model
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
base_model.trainable = False  # Freeze the base model

# Create new model on top
x = GlobalAveragePooling2D()(base_model.output)
x = Dense(1024, activation='relu')(x)
output = Dense(3, activation='softmax')(x)  # Assuming you have 3 classes
model2 = Model(inputs=base_model.input, outputs=output)

# Specify the learning rate
learning_rate = 0.0001  # You can change this value to whatever you prefer

# Compile the model using an optimizer object with a specified learning rate
model2.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

history2 = model2.fit(
    train_dataset,
    epochs=10,
    validation_data=validation_dataset
)

# Plotting the training and validation loss
plt.figure(figsize=(8, 5))
plt.plot(history2.history['loss'], label='Training Loss')
plt.plot(history2.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss Per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

# Plotting the training and validation accuracy
plt.figure(figsize=(8, 5))
plt.plot(history2.history['accuracy'], label='Training Accuracy')  # Use 'acc' instead of 'accuracy' if using TensorFlow 1.x
plt.plot(history2.history['val_accuracy'], label='Validation Accuracy')  # Use 'val_acc' if using TensorFlow 1.x
plt.title('Training and Validation Accuracy Per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

# Sample prediction (replace with your actual test data loading if it's different)
y_pred = model2.predict(validation_dataset)
y_pred_classes = y_pred.argmax(axis=-1)  # Get the predicted class labels

validation_dataset_values = []
for x, y in validation_dataset:
  validation_dataset_values.append(y.numpy())

validation_dataset_values = np.concatenate(validation_dataset_values)

from sklearn.metrics import confusion_matrix
import pandas as pd

# Create the confusion matrix
cm = confusion_matrix(validation_dataset_values, y_pred_classes)

# Convert the confusion matrix to a DataFrame
df_cm = pd.DataFrame(cm, columns=['Infant', 'Parent', 'Other'], index=['Actual Infant', 'Actual Parent', 'Actual Other'])

# Display the confusion matrix
print(df_cm)

import seaborn as sns

conf_matrix = confusion_matrix(y_pred_classes, validation_dataset_values)
# Plot confusion matrix
plt.figure(figsize=(3, 3))
sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix for Audio Classification Model')
plt.show()

"""Model 3 - Even lower learning rate"""

# Load ResNet50 base model
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
base_model.trainable = False  # Freeze the base model

# Create new model on top
x = GlobalAveragePooling2D()(base_model.output)
x = Dense(1024, activation='relu')(x)
output = Dense(3, activation='softmax')(x)  # Assuming you have 3 classes
model3 = Model(inputs=base_model.input, outputs=output)

# Specify the learning rate
learning_rate = 0.00001  # You can change this value to whatever you prefer

# Compile the model using an optimizer object with a specified learning rate
model3.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

history3 = model3.fit(
    train_dataset,
    epochs=10,
    validation_data=validation_dataset
)

# Plotting the training and validation loss
plt.figure(figsize=(8, 5))
plt.plot(history3.history['loss'], label='Training Loss')
plt.plot(history3.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss Per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(False)
plt.show()

# Plotting the training and validation accuracy
plt.figure(figsize=(8, 5))
plt.plot(history3.history['accuracy'], label='Training Accuracy')  # Use 'acc' instead of 'accuracy' if using TensorFlow 1.x
plt.plot(history3.history['val_accuracy'], label='Validation Accuracy')  # Use 'val_acc' if using TensorFlow 1.x
plt.title('Training and Validation Accuracy Per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(False)
plt.show()

# Sample prediction (replace with your actual test data loading if it's different)
y_pred = model3.predict(validation_dataset)
y_pred_classes = y_pred.argmax(axis=-1)  # Get the predicted class labels

validation_dataset_values = []
for x, y in validation_dataset:
  validation_dataset_values.append(y.numpy())

validation_dataset_values = np.concatenate(validation_dataset_values)

from sklearn.metrics import confusion_matrix
import pandas as pd

# Create the confusion matrix
cm = confusion_matrix(validation_dataset_values, y_pred_classes)

# Convert the confusion matrix to a DataFrame
df_cm = pd.DataFrame(cm, columns=['Infant', 'Parent', 'Other'], index=['Actual Infant', 'Actual Parent', 'Actual Other'])

# Display the confusion matrix
print(df_cm)

import seaborn as sns

conf_matrix = confusion_matrix(y_pred_classes, validation_dataset_values)
# Plot confusion matrix
plt.figure(figsize=(3, 3))
sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix for Audio Classification Model')
plt.show()